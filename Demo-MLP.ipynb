{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad2405b",
   "metadata": {},
   "source": [
    "# Demo of MLP Library Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8080a",
   "metadata": {},
   "source": [
    "## INSTRUCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c31d24",
   "metadata": {},
   "source": [
    "For more detailed instructions of using MLPLibrary consult the documentation. This demo will give practical demonstration of MLPLibrary functions and usage for constructing MLP models, setting hyperparameters and evaluating training and testing performance, using the provided data for COMP5329 Assignment 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332c0e07",
   "metadata": {},
   "source": [
    "### Installing MLPLibrary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5885a23",
   "metadata": {},
   "source": [
    "As provided in the README.md in the main folder, MLPlibrary may be installed locally as a portable library (similar to NumPy or PyTorch) simply by navigating into the network directory (i.e. ```'MLPLibrary/network/'```) and executing the following statement:\n",
    "```\n",
    "pip install ..\n",
    "```\n",
    "This will install all the required dependencies stated in the ```setup.py``` file and allow usage of our library functions anywhere on your device locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f97264",
   "metadata": {},
   "source": [
    "After this step, you may perform the following imports. If you decide not to install MLPLibrary, make sure you are within the main folder of MLPLibrary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e4fa07",
   "metadata": {},
   "source": [
    "## IMPORTING MLPLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the modules and functions related to constructing MLP network models\n",
    "\n",
    "from network.net import Net                   # Net class, base class for constructing MLP networks\n",
    "from network.layer import Linear              # Linear class, child class of parent class Layer \n",
    "from network.loss import CrossEntropyLoss     # CrossEntropyLoss class, child class of parent class Loss\n",
    "from network.activ import ReLU, LeakyReLU     # ReLU, LeakyReLU classes, child classes of parent class Activation\n",
    "from network.optim import SGD, Adam           # SGD, Adam classes, child classes of parent class Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the modules and functions related to data processing including loaders for the assignment data\n",
    "\n",
    "# Process module contains functions relating to data processing:\n",
    "from network.loader.process import (\n",
    "    train_test_split,        # Function to split data with chosen ratio, data can be shuffled\n",
    "    normalize,               # Normalizes data to have mean of zero and unit variance\n",
    "    standardize,             # Normalizes data to be between range 0-1, i.e. standardizes data\n",
    "    one_hot,                 # One hot encoding: 100% prob of 2 is [0, 0, 1] with 3 classes\n",
    "    pca                      # Reduces data to chosen K principal components\n",
    ") \n",
    "\n",
    "# Data module for loading the assignment data\n",
    "from network.dataset.source import (\n",
    "    get_data_from_file,   # Loads assignment data from file (must be within main directory)\n",
    "    get_data_from_url     # Loads assignment data from public GitHub repo that stores data\n",
    ")\n",
    "\n",
    "# Data loader module for automating processing of and loading of assignment data based on parameter selections\n",
    "from network.loader.data_loader import load_train_val_test  # Parameter selections decide method of processing                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c176d",
   "metadata": {},
   "source": [
    "### Importing Standard Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d13889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "# setting random seed\n",
    "np.random.seed(88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6951b7",
   "metadata": {},
   "source": [
    "### Example Data Loading and Preprocessing\n",
    "\n",
    "#### Parameter Selections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DATA = \"url\"          # May choose from \"url\" or \"file\" to source assignment data (must be in main for \"file\")\n",
    "NORM_METHOD = \"standardize\"  # May choose from \"standardize\", \"normalize\" or \"none\" (none is no normalization on data)\n",
    "PCA_N_COMPONENTS = 0         # If PCA_N_COMPONENTS > 0;\n",
    "                                # Normalization is skipped as done implicitly by mean centering prior to applying PCA\n",
    "                                # Strictly, the choice of PCA_N_COMPONENTS <= N_DIMENSIONS of the input dataset \n",
    "N_CATEGORIES = 10            # If N_CATEGORIES > 0 chosen, then categorical one-hot encoding is applied to label data\n",
    "SPLIT_RATIO = 0.2             # Ratio data is to be split upon when obtaining a train test split (default is set to 0.2)\n",
    "SHUFFLE_DATA = True          # IF SHUFFLE is True, then the data is shuffled prior to splitting by taking random indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754fd715",
   "metadata": {},
   "source": [
    "#### Obtaining loaded and processed data based on the chosen parameter selections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note as we are loading data from URL it will take longer than from file.\n",
    "train_set, valid_set, test_set = load_train_val_test(\n",
    "    source = SOURCE_DATA,\n",
    "    method = NORM_METHOD,      \n",
    "    pca_N = PCA_N_COMPONENTS,\n",
    "    n_categories = N_CATEGORIES,\n",
    "    ratio = SPLIT_RATIO,\n",
    "    shuffle = SHUFFLE_DATA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198bb38c",
   "metadata": {},
   "source": [
    "With a ratio of 0.2 and shuffle selected, the validation set is split from the training set by shuffling the data and taking random indices in proportion of the ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_sep = \"------------------------------------------------\"\n",
    "print(line_sep)\n",
    "print(f\"Shape of Training Data: {train_set[0].shape}\")\n",
    "print(f\"Shape of Training Labels: {train_set[1].shape}\")\n",
    "print(line_sep)\n",
    "print(f\"Shape of Validation Data: {valid_set[0].shape}\")\n",
    "print(f\"Shape of Validation Labels: {valid_set[1].shape}\")\n",
    "print(line_sep)\n",
    "print(f\"Shape of Test Data: {test_set[0].shape}\")\n",
    "print(f\"Shape of Test Labels: {test_set[1].shape}\")\n",
    "print(line_sep + '\\n')\n",
    "print(f\"First 10 one-hot encoded labels of test data:\\n\\n{test_set[1][:10]}\")\n",
    "print(f\"\\nEquivalent to: {[np.argmax(label) for label in test_set[1][:10]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c61ab",
   "metadata": {},
   "source": [
    "## CONSTRUCTING MLP MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fa068",
   "metadata": {},
   "source": [
    "### OPTIMIZER SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b4809",
   "metadata": {},
   "source": [
    "##### Optimizer 1: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.04\n",
    "WEIGHT_DECAY = 0.001\n",
    "MOMENTUM_TERM = 0.999\n",
    "LR_SCHEDULER = \"step\"\n",
    "STEP_TERMS = (25, .95)  # lr dropped by factor of 5% each step of 25 epochs\n",
    "\n",
    "sgd = SGD(\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    weight_decay = WEIGHT_DECAY,\n",
    "    momentum = MOMENTUM_TERM,\n",
    "    lr_decay = LR_SCHEDULER,\n",
    "    step_terms = STEP_TERMS,\n",
    ")\n",
    "print(f\"Optimizer: {sgd} is initialized and ready to be deployed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ac741",
   "metadata": {},
   "source": [
    "##### Optimizer 2: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0eb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0008    # can set even smaller than default 0.001 (can hopefully increase performance)\n",
    "BETA_TERM_ONE = 0.900            # this is allowed in adaptive learning rate methods such as Adam\n",
    "BETA_TERM_TWO = 0.999       # will take longer to converge, however, may reach a lower validation loss\n",
    "EPS_STABILITY = 1e-09 # usually a sharp loss is seen when lr = 0.001 which is good to see but doesn't produce BEST results\n",
    "\n",
    "adam = Adam(\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    beta1 = BETA_TERM_ONE,\n",
    "    beta2 = BETA_TERM_TWO,\n",
    "    epsilon = EPS_STABILITY\n",
    ")\n",
    "print(f\"Optimizer: {adam} is initialized and ready to be deployed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055ce5a",
   "metadata": {},
   "source": [
    "### MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3294055",
   "metadata": {},
   "source": [
    "##### Network Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = sgd\n",
    "CRITERION = CrossEntropyLoss()\n",
    "BATCH_NORM = True\n",
    "ALPHA_TERM = 0.9\n",
    "L2_REG_TERM = 0.004\n",
    "\n",
    "mlp = Net(\n",
    "    optimizer = OPTIMIZER,\n",
    "    criterion = CRITERION,\n",
    "    batch_norm = BATCH_NORM,\n",
    "    alpha = ALPHA_TERM,\n",
    "    L2_reg_term = L2_REG_TERM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6af2d9e",
   "metadata": {},
   "source": [
    "##### Adding Hidden Linear and Activation Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd2cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.add(Linear(indim = 128, outdim = 1024, dropout = 0.2))\n",
    "mlp.add(ReLU())\n",
    "mlp.add(Linear(indim = 1024, outdim = 64, dropout = 0.2))\n",
    "mlp.add(ReLU())\n",
    "mlp.add(Linear(indim = 64, outdim = 32, dropout = 0.2))\n",
    "mlp.add(ReLU())\n",
    "mlp.add(Linear(indim = 32, outdim = 10))\n",
    "\n",
    "mlp.set_name(\"SGD_small_network\")\n",
    "print(f\"{mlp.model_name} is initialized and ready to be trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab1f87",
   "metadata": {},
   "source": [
    "### Plotting Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ade171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(stats):\n",
    "    ep, tl, ta, vl, va = stats\n",
    "    pl.figure(figsize = (10, 7))\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = pl.subplots(2, 2)\n",
    "    fig.suptitle(f'Training Results, best model found @ Epoch {ep}')\n",
    "\n",
    "    ax1.plot(tl)\n",
    "    ax1.set_title('Training Loss')\n",
    "\n",
    "    ax2.plot(vl, 'tab:orange')\n",
    "    ax2.set_title('Validation Loss')\n",
    "\n",
    "    ax3.plot(ta, 'tab:green')\n",
    "    ax3.set_title('Training Accuracy')\n",
    "\n",
    "    ax4.plot(va, 'tab:red')\n",
    "    ax4.set_title('Validation Accuracy')\n",
    "    \n",
    "    for ax in fig.get_axes():\n",
    "        ax.label_outer()\n",
    "\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5ec63",
   "metadata": {},
   "source": [
    "## NETWORK TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dcd712",
   "metadata": {},
   "source": [
    "### TRAINING TILL CONVERGENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENT_CHANGE_IN_LOSS = 1e-25    # convergence criteria has been achieved when:\n",
    "CHECK_LAST_N_MODELS = 10            # (1) training loss is not reduced in next model by chosen N percentage change; OR\n",
    "BATCH_SIZE = 500                    # (2) if min val loss since last best model is not beat by any of the next M models\n",
    "MAX_EPOCHS = 200                  \n",
    "REPORTING_INTERVAL = 1\n",
    "\n",
    "stats = mlp.train_convergence(\n",
    "    train_set = train_set,\n",
    "    valid_set = valid_set,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    planned_epochs = MAX_EPOCHS,\n",
    "    last_check = CHECK_LAST_N_MODELS,\n",
    "    threshold = PERCENT_CHANGE_IN_LOSS,\n",
    "    report_interval = REPORTING_INTERVAL\n",
    ")\n",
    "\n",
    "{\"tags\": [\"hide-output\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f58e2",
   "metadata": {},
   "source": [
    "### Plotting Epoch-wise Loss and Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32577a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55546543",
   "metadata": {},
   "source": [
    "### Checking Accuracy of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c074b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading best model found:\n",
    "\n",
    "best_model = Net.load_model(\"model/\" + mlp.model_name)\n",
    "best_model.test_network(train_set, \"train data\")\n",
    "best_model.test_network(valid_set, \"valid data\")\n",
    "best_model.test_network(test_set, \"test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c58de12",
   "metadata": {},
   "source": [
    "### Can We Do Better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e228b1",
   "metadata": {},
   "source": [
    "Let's nudge training to see if we can obtain a better model in the next 50 epochs. Learning rate is also reset, this could allow us to get out of a local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a467c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.set_name(\"SGD_small_network_alt\")\n",
    "\n",
    "stats = best_model.train_network(\n",
    "    train_set = train_set,\n",
    "    valid_set = valid_set,\n",
    "    batch_size = 250,        # reduced batch size to vary any possible patterns learned\n",
    "    epochs = 50,\n",
    "    report_interval = 1\n",
    ")\n",
    "\n",
    "{\"tags\": [\"hide-output\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64230e2f",
   "metadata": {},
   "source": [
    "### Plotting Epoch-wise Loss and Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cb9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ee1a1",
   "metadata": {},
   "source": [
    "### Let's see how our new model does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a0bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading new best model found:\n",
    "\n",
    "new_model = Net.load_model(\"model/SGD_small_network_alt\")\n",
    "new_model.test_network(train_set, \"train data\")\n",
    "new_model.test_network(valid_set, \"valid data\")\n",
    "new_model.test_network(test_set, \"test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06864f2d",
   "metadata": {},
   "source": [
    "### Confusion Matrix Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fde1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(pred, label):\n",
    "    x, y = len(np.unique(pred)), len(np.unique(label))\n",
    "    matrix = np.zeros((x, y))\n",
    "    for i in range(len(pred)):\n",
    "        m, n = pred[i], label[i]\n",
    "        matrix[m, n] += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13defc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix of training data\n",
    "pred = new_model.predict(train_set[0], train_set[1].shape[1])\n",
    "pred_train_labels = np.argmax(pred, axis=1)\n",
    "\n",
    "matrix = confusion_matrix(pred_train_labels, np.argmax(train_set[1], axis=1))\n",
    "matrix_df = pd.DataFrame(matrix, index = np.arange(10), columns = np.arange(10))\n",
    "\n",
    "pl.figure(figsize = (10,7))\n",
    "sns.heatmap(matrix, annot=True)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb4ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix on test data\n",
    "pred = new_model.predict(test_set[0], test_set[1].shape[1])\n",
    "pred_test_labels = np.argmax(pred, axis=1)\n",
    "\n",
    "matrix = confusion_matrix(pred_test_labels, np.argmax(test_set[1], axis=1))\n",
    "matrix_df = pd.DataFrame(matrix, index = np.arange(10), columns = np.arange(10))\n",
    "\n",
    "pl.figure(figsize = (10,7))\n",
    "sns.heatmap(matrix, annot=True)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted labels:', pred_test_labels.shape, '\\n', pred_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b274c",
   "metadata": {},
   "source": [
    "## Using Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = adam\n",
    "L2_REG_TERM = 0.001\n",
    "\n",
    "\n",
    "mlp = Net(\n",
    "    optimizer = OPTIMIZER,\n",
    "    criterion = CRITERION,\n",
    "    batch_norm = BATCH_NORM,\n",
    "    alpha = ALPHA_TERM,\n",
    "    L2_reg_term = L2_REG_TERM\n",
    ")\n",
    "\n",
    "mlp.add(Linear(128, 1024, dropout=0.4))\n",
    "mlp.add(ReLU())\n",
    "mlp.add(Linear(1024, 512, dropout=0.2))\n",
    "mlp.add(ReLU())\n",
    "mlp.add(Linear(512, 64, dropout=0.2))\n",
    "mlp.add(ReLU())\n",
    "mlp.add(Linear(64, 16, dropout=0.2))\n",
    "mlp.add(ReLU())\n",
    "mlp.add(Linear(16, 10))  \n",
    "\n",
    "\n",
    "mlp.set_name(\"Adam_network\")\n",
    "print(f\"{mlp.model_name} is initialized and ready to be trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438bc2c",
   "metadata": {},
   "source": [
    "### TRAINING TILL CONVERGENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = mlp.train_convergence(\n",
    "    train_set = train_set,\n",
    "    valid_set = valid_set,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    planned_epochs = MAX_EPOCHS,\n",
    "    last_check = CHECK_LAST_N_MODELS,\n",
    "    threshold = PERCENT_CHANGE_IN_LOSS,\n",
    "    report_interval = REPORTING_INTERVAL\n",
    ")\n",
    "\n",
    "{\"tags\": [\"hide-output\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbcd928",
   "metadata": {},
   "source": [
    "### Plotting Epoch-wise Loss and Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54907",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c2fdb",
   "metadata": {},
   "source": [
    "### Checking Accuracy of Best Model Found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading best model found:\n",
    "\n",
    "best_model = Net.load_model(\"model/\" + mlp.model_name)\n",
    "best_model.test_network(train_set, \"train data\")\n",
    "best_model.test_network(valid_set, \"valid data\")\n",
    "best_model.test_network(test_set, \"test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39dd009",
   "metadata": {},
   "source": [
    "### Can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63190e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.set_name(\"Adam_network_alt\")\n",
    "\n",
    "stats = best_model.train_network(\n",
    "    train_set = train_set,\n",
    "    valid_set = valid_set,\n",
    "    batch_size = 250,        # reduced batch size to vary any possible patterns learned\n",
    "    epochs = 50,\n",
    "    report_interval = 1\n",
    ")\n",
    "\n",
    "{\"tags\": [\"hide-output\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae57740",
   "metadata": {},
   "source": [
    "### Is there potential for improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d7add",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6904226",
   "metadata": {},
   "source": [
    "Continuing training for 10 more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading \"new best model found\":\n",
    "\n",
    "new_model = Net.load_model(\"model/Adam_network_alt\")\n",
    "new_model.test_network(train_set, \"train data\")\n",
    "new_model.test_network(valid_set, \"valid data\")\n",
    "new_model.test_network(test_set, \"test data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde283e",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c94d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the confusion matrix of training data\n",
    "pred = new_model.predict(train_set[0], train_set[1].shape[1])\n",
    "pred_train_labels = np.argmax(pred, axis=1)\n",
    "\n",
    "matrix = confusion_matrix(pred_train_labels, np.argmax(train_set[1], axis=1))\n",
    "matrix_df = pd.DataFrame(matrix, index = np.arange(10), columns = np.arange(10))\n",
    "\n",
    "pl.figure(figsize = (10,7))\n",
    "sns.heatmap(matrix, annot=True)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix on test data\n",
    "pred = new_model.predict(test_set[0], test_set[1].shape[1])\n",
    "pred_test_labels = np.argmax(pred, axis=1)\n",
    "\n",
    "matrix = confusion_matrix(pred_test_labels, np.argmax(test_set[1], axis=1))\n",
    "matrix_df = pd.DataFrame(matrix, index = np.arange(10), columns = np.arange(10))\n",
    "\n",
    "pl.figure(figsize = (10,7))\n",
    "sns.heatmap(matrix, annot=True)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c84034",
   "metadata": {},
   "source": [
    "##### Continuing training for 10 more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.set_name(\"Adam_network_alt\")\n",
    "\n",
    "stats = best_model.train_network(\n",
    "    train_set = train_set,\n",
    "    valid_set = valid_set,\n",
    "    batch_size = 250,        # reduced batch size to vary any possible patterns learned\n",
    "    epochs = 10,\n",
    "    report_interval = 1\n",
    ")\n",
    "\n",
    "{\"tags\": [\"hide-output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(stats)\n",
    "\n",
    "new_model = Net.load_model(\"model/Adam_network_alt\")\n",
    "new_model.test_network(train_set, \"train data\")\n",
    "new_model.test_network(valid_set, \"valid data\")\n",
    "new_model.test_network(test_set, \"test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67beaa",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
