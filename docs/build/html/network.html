<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>network package &mdash; MLPLibrary 0.0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/style.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="network.dataset package" href="network.dataset.html" />
    <link rel="prev" title="network" href="modules.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MLPLibrary
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Setup and Introduction:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installing.html">Setup &amp; Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guide to MLPLibrary:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mlplibrary-guide.html">MLPLibrary Features and Functionality</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Modules</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">network</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">network package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#subpackages">Subpackages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="network.dataset.html">network.dataset package</a></li>
<li class="toctree-l4"><a class="reference internal" href="network.loader.html">network.loader package</a></li>
<li class="toctree-l4"><a class="reference internal" href="network.model.html">network.model package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-network.activ">network.activ module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-network.layer">network.layer module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-network.loss">network.loss module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-network.net">network.net module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-network.optim">network.optim module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-network">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MLPLibrary</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="modules.html">network</a> &raquo;</li>
      <li>network package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/network.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="network-package">
<h1>network package<a class="headerlink" href="#network-package" title="Permalink to this headline"></a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="network.dataset.html">network.dataset package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="network.dataset.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="network.dataset.html#module-network.dataset.source">network.dataset.source module</a></li>
<li class="toctree-l2"><a class="reference internal" href="network.dataset.html#module-network.dataset">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="network.loader.html">network.loader package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="network.loader.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="network.loader.html#module-network.loader.data_loader">network.loader.data_loader module</a></li>
<li class="toctree-l2"><a class="reference internal" href="network.loader.html#module-network.loader.process">network.loader.process module</a></li>
<li class="toctree-l2"><a class="reference internal" href="network.loader.html#module-network.loader">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="network.model.html">network.model package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="network.model.html#module-network.model">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline"></a></h2>
</section>
<section id="module-network.activ">
<span id="network-activ-module"></span><h2>network.activ module<a class="headerlink" href="#module-network.activ" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="network.activ.Activation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">network.activ.</span></span><span class="sig-name descname"><span class="pre">Activation</span></span><a class="headerlink" href="#network.activ.Activation" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Parent class for Non-Linear Activation Functions</p>
<p>This class is inherited by currently supported child classes: ReLU and LeakyReLU.</p>
<dl class="py method">
<dt class="sig sig-object py" id="network.activ.Activation.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dy</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.activ.Activation.backward" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.activ.Activation.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.activ.Activation.forward" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="network.activ.LeakyReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">network.activ.</span></span><span class="sig-name descname"><span class="pre">LeakyReLU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">leak</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.03</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.activ.LeakyReLU" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#network.activ.Activation" title="network.activ.Activation"><code class="xref py py-class docutils literal notranslate"><span class="pre">network.activ.Activation</span></code></a></p>
<p>Class for Activation function: Leaky Rectified Linear Unit (LeakyReLU)</p>
<p>Output is then scaled by multiplication with dy in the backward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>leak</strong> (<em>float</em>) – the leaky amount (slope) in Leaky ReLU function (usually very small).             Defaults to 0.03 (arbitrarily chosen amount).</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="network.activ.LeakyReLU.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dy</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.activ.LeakyReLU.backward" title="Permalink to this definition"></a></dt>
<dd><p>Passes the gradient of the loss (cost / error) function w.r.t weights in
previous layers and applies the derivative of the ReLU function in processing
input for the backward pass of backpropagation to update weights and biases.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dy</strong> (<em>np.ndarray</em>) – gradient of the loss function in backward pass</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns derivative of ReLU applied to dy and cached x in forward pass.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.activ.LeakyReLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.activ.LeakyReLU.forward" title="Permalink to this definition"></a></dt>
<dd><p>Calculate Leaky ReLU-fied output of passed in mini-batch data (x) in forward pass.
This function is activated during both training and prediction. This is then
fed into the next layer (most likely a Linear layer).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>np.ndarray</em>) – mini-batch data passed to network in forward pass</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns Leaky ReLU-fied output of the mini-batch data</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="network.activ.ReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">network.activ.</span></span><span class="sig-name descname"><span class="pre">ReLU</span></span><a class="headerlink" href="#network.activ.ReLU" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#network.activ.Activation" title="network.activ.Activation"><code class="xref py py-class docutils literal notranslate"><span class="pre">network.activ.Activation</span></code></a></p>
<p>Class for Activation function: Rectified Linear Unit (ReLU)</p>
<p>Output is then scaled by multiplication with dy in the backward pass.</p>
<dl class="py method">
<dt class="sig sig-object py" id="network.activ.ReLU.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dy</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.activ.ReLU.backward" title="Permalink to this definition"></a></dt>
<dd><p>Passes the gradient of the loss (cost / error) function w.r.t weights in
previous layers and applies the derivative of the ReLU function in processing
input for the backward pass of backpropagation to update weights and biases.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dy</strong> (<em>np.ndarray</em>) – gradient of the loss function in backward pass</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns derivative of ReLU applied to dy and cached x in forward pass.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.activ.ReLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.activ.ReLU.forward" title="Permalink to this definition"></a></dt>
<dd><p>Calculates ReLU-fied output of passed in mini-batch data (x) in forward pass.
This function is activated during both training and prediction. This is then
fed into the next layer (most likely a Linear layer).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>np.ndarray</em>) – mini-batch data passed to network in forward pass</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns ReLU-fied output of the mini-batch data</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-network.layer">
<span id="network-layer-module"></span><h2>network.layer module<a class="headerlink" href="#module-network.layer" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="network.layer.Layer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">network.layer.</span></span><span class="sig-name descname"><span class="pre">Layer</span></span><a class="headerlink" href="#network.layer.Layer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Parent class for Hidden Layers</p>
<p>This class is inherited by currently only supported child class: Linear</p>
<dl class="py method">
<dt class="sig sig-object py" id="network.layer.Layer.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Layer.backward" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.layer.Layer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Layer.forward" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.layer.Layer.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Layer.update" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="network.layer.Linear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">network.layer.</span></span><span class="sig-name descname"><span class="pre">Linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outdim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'xavier'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zero'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Linear" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#network.layer.Layer" title="network.layer.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">network.layer.Layer</span></code></a></p>
<p>Class for Linear Layer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indim</strong> (<em>int</em>) – input dimensions of mini-batch data or number of dimensions of input feature map</p></li>
<li><p><strong>outdim</strong> (<em>int</em>) – output dimensions or number of hidden neurons within linear layer to process input</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – value between 0 and 1 to indicate ratio of neurons to randomly deactivate in layer</p></li>
<li><p><strong>weights</strong> (<em>str</em>) – weights init method with typical selection from “xavier” and “kaiming” (defaults to “xavier”)’</p></li>
<li><p><strong>bias</strong> (<em>str</em>) – bias init method with selection from above and either “zero” or “const” (defaults to “zero”)</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="network.layer.Linear.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dy</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Linear.backward" title="Permalink to this definition"></a></dt>
<dd><p>Main method for Layer in making a backward pass for backpropagation. Order of applying the gradients
of dropout and batch norm are applied in the order stated and is a mirror application of the forward pass.
The derivative of the weight values are calculated as the dot product of loss derivative with cached input
x (that is matrix rotated / transposed) and for bias, the sum of the gradient values of the loss function.
Lastly, derivative of the mini-batch x is returned as the dot product between rotated gradient of weights
multiplied against the the derivative of the loss function values. If L2 regularization is applied (with
value determined upon the initialization of Net), then the derivative of weights reduced by a factor of this
term, the value of weights and the size of mini-batch x. This is understood to penalize larger weight values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dy</strong> (<em>np.ndarray</em>) – derivative of the loss function passed in via back-propagation.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>derivative of the loss function w.r.t. the weights of this layer.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.layer.Linear.batch_norm_backward">
<span class="sig-name descname"><span class="pre">batch_norm_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dy</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Linear.batch_norm_backward" title="Permalink to this definition"></a></dt>
<dd><p>Helper function for the backward pass of batch normalization using the derivative of the loss
function. Partial derivatives for shift values of beta are calculated as the sum of all values over the
input derivative of the loss function and derivatives for for gamma using the chain rule as the sum of
all normalized values pre-cached values in forward pass multiplied against the derivative of loss.
This is then used to calculate the derivate for the mini-batch x w.r.t. the loss function and returns
it’s batch normalized value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dy</strong> (<em>np.ndarray</em>) – derivative of the loss function passed in via back-propagation.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns batch-normalized value for the derivative of the loss function in this layer</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.layer.Linear.batch_norm_forward">
<span class="sig-name descname"><span class="pre">batch_norm_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Linear.batch_norm_forward" title="Permalink to this definition"></a></dt>
<dd><p>Helper function for a forward pass using batch normalization on mini-batch x.
If making a prediction (i.e. not undergoing backpropagation and only making a forward pass),
then the running means and variance calculated in this layer from training are then used
instead to normalise the output of this batch for the next layer. The predict function is
flagged True in the Net class upon calling the Net.predict() method on the network, this
sets all layer.predict attributes to True to determine to use previously calculated means
and variance for normalizing x, or if False (during training) to calculate this mean and variance.</p>
<p>Gamma and beta are initialized during initialization of Linear layer to scale amd shift our
normalized value of x, alpha is a scalar term set upon Net initialization to scale the running mean and variance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>np.ndarray</em>) – mini-batch x during forward pass of training or prediction.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns normalized mini-batch x, that is scaled with gamma and shifted with beta</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.layer.Linear.dropout_backward">
<span class="sig-name descname"><span class="pre">dropout_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dy</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Linear.dropout_backward" title="Permalink to this definition"></a></dt>
<dd><p>Helper function for applying dropout to the backward pass on the derivative for the loss function. Using
the cached mask from forward pass in this layer, applies this mask similarly to the derivative of loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dy</strong> (<em>np.ndarray</em>) – derivative of the loss function passed in via back-propagation.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns dx values whereby proportion (dropout) is randomly masked as zeros, turned off.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.layer.Linear.dropout_forward">
<span class="sig-name descname"><span class="pre">dropout_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Linear.dropout_forward" title="Permalink to this definition"></a></dt>
<dd><p>Helper function for applying dropout to the forward pass in either training or prediction. If making a
prediction, dropout is deactivated. During training, dropout randomly chooses neurons to drop in the
linear transformation of x in the forward pass w.r.t. each neuron’s weight and bias. Dropout parameter
is initialized upon Layer initialization, and used to drop a percentage of the neuron’s in this Layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>np.ndarray</em>) – mini-batch x during forward pass of training or prediction.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns new x values whereby proportion (dropout) is randomly masked as zeros, turned off.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.layer.Linear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Linear.forward" title="Permalink to this definition"></a></dt>
<dd><p>Main method for Layer in making a forward pass in training / prediction. Using linear transformation
on mini-batch x to calculate an output w.r.t. learned weights and biases from backpropagation. This
transformation: y = wx + b; takes mini-batch x as input, scales it by learned weight values, and shifts
it by learned bias values to calculate output y to be used in the next layer. Ordering of applying both
batch-norm and dropout is calculated in the order stated, when both are set for use within the Layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>np.ndarray</em>) – mini-batch x during forward pass of training or prediction.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns linearly transformed x values as input to next layer (i.e. for activation)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.layer.Linear.kaiming">
<span class="sig-name descname"><span class="pre">kaiming</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Linear.kaiming" title="Permalink to this definition"></a></dt>
<dd><p>Function for Kaiming-He Uniform Initialization, primarily used for randomly initializing
weight values in a layer, however, can also be used for biases as well.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>tuple</em>) – dimensions of generated np.ndarray i.e. (#row, #col)</p></li>
<li><p><strong>gain</strong> (<em>int</em>) – scalar multiplication of generated weight/bias values</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns weights/bias of given size (and gain) using Kaiming-He initialization</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.layer.Linear.reset_gradients">
<span class="sig-name descname"><span class="pre">reset_gradients</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Linear.reset_gradients" title="Permalink to this definition"></a></dt>
<dd><p>Method to reset the gradient of the weight and bias values for each Layer, called upon
after each iteration of batch training.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.layer.Linear.xavier">
<span class="sig-name descname"><span class="pre">xavier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.layer.Linear.xavier" title="Permalink to this definition"></a></dt>
<dd><p>Function for Xavier Uniform Initialization, primarily used for randomly initializing
weight values in a layer, however, can be used for biases as well.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>tuple</em>) – dimensions of generated np.ndarray i.e. (#row, #col)</p></li>
<li><p><strong>gain</strong> (<em>int</em>) – scalar multiplication of generated weight/bias values</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns weights/bias of given size (and gain) using Xavier initialization</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-network.loss">
<span id="network-loss-module"></span><h2>network.loss module<a class="headerlink" href="#module-network.loss" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="network.loss.CrossEntropyLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">network.loss.</span></span><span class="sig-name descname"><span class="pre">CrossEntropyLoss</span></span><a class="headerlink" href="#network.loss.CrossEntropyLoss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#network.loss.Loss" title="network.loss.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">network.loss.Loss</span></code></a></p>
<p>Class for Cross-Entropy Loss. By default, this class applies Softmax activation on input mini-batch x
to calculate probabilities for each class. These quantized probabilities for each class are then used
in the calculation of Cross-Entropy Loss for backpropagation (training) of the network, or directly in
making a prediction by taking the class with the maximum Softmax calculated probability.</p>
<dl class="py method">
<dt class="sig sig-object py" id="network.loss.CrossEntropyLoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.loss.CrossEntropyLoss.backward" title="Permalink to this definition"></a></dt>
<dd><p>The backward pass of Cross-Entropy Loss calculates the derivative of Cross-Entropy Loss (loss function)
using Softmax activation. Both Softmax probabilities and ground-truth labels are cached for the current
mini-batch in the forward pass and used to calculate the derivative in the backward pass. This is simply
the difference of probabilities with one-hot encoded ground-truth labels averaged over all the samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>derivative of Cross-Entropy Loss with Softmax activation averaged number of samples in mini-batch x.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.loss.CrossEntropyLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.loss.CrossEntropyLoss.forward" title="Permalink to this definition"></a></dt>
<dd><p>The forward step in Cross-Entropy Loss is the calculation of the Cross-Entropy Loss value,
determined by calculating Softmax probabilities for each class and taking the log-likelihood of
probabilities which determines the negative average of log corrected predicted probabilities for
the mini-batch x (ie. against true labels). The probabilities calculated with Softmax are cached
along with ground-truth labels, to then be used for calculating the derivative of Cross-Entropy
for the backward pass for updating weights/biases in each layer. Softmax probabilities are also
returned for use in evaluating the accuracy of the model, or making predictions on unlabelled data.
Epsilon is used to stabilize calculation of log Softmax where unwanted overflow/underflow side-effects
could occur due to floating-point representation of small probability values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>np.ndarray</em>) – mini-batch x of values passed through in the forward pass of training / prediction</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns the calculated cross-entropy loss over mini-batch x;
np.ndarray: also returns Softmax probabilities for each class for prediction / accuracy evaluation</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.loss.CrossEntropyLoss.softmax">
<span class="sig-name descname"><span class="pre">softmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.loss.CrossEntropyLoss.softmax" title="Permalink to this definition"></a></dt>
<dd><p>Used for calculating the Softmax probabilities of each class. Acts as the default last activation in
the layer prior to calculation of Cross Entropy Loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>np.ndarray</em>) – mini-batch x of values passed through in the forward pass of training / prediction</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns the probabilities for each class, the class with the highest probability is our predicted label</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="network.loss.Loss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">network.loss.</span></span><span class="sig-name descname"><span class="pre">Loss</span></span><a class="headerlink" href="#network.loss.Loss" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Parent class for Loss Criterion</p>
<p>This class is inherited by currently only supported child class: CrossEntropyLoss</p>
<dl class="py method">
<dt class="sig sig-object py" id="network.loss.Loss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.loss.Loss.backward" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.loss.Loss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.loss.Loss.forward" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-network.net">
<span id="network-net-module"></span><h2>network.net module<a class="headerlink" href="#module-network.net" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="network.net.Net">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">network.net.</span></span><span class="sig-name descname"><span class="pre">Net</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">SGD(mm)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">CELoss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">L2_reg_term</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class for Neural Networks (Multi-Layer Perceptron models).
Currently supports: Hidden Layers (Linear) and Activation functions (ReLU, LeakyReLU); Loss Criterion:
CrossEntropyLoss (implicitly with Softmax activation); Optimizers: SGD (with or without Momentum and/or
Weight decay) and Adam; Regularization: L2 Regularization over network, Dropout (specific to each layer)
and Batch Normalization (toggled for whole network for ease of convenience).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<a class="reference internal" href="#network.optim.Optimizer" title="network.optim.Optimizer"><em>Optimizer</em></a>) – Optimizer to be applied for updating weights and biases in each layer during training.</p></li>
<li><p><strong>criterion</strong> (<a class="reference internal" href="#network.loss.Loss" title="network.loss.Loss"><em>Loss</em></a>) – Loss criterion used for the evaluation and optimization of minimizing the loss function.</p></li>
<li><p><strong>batch_norm</strong> (<em>bool</em>) – Flag to toggle application of batch normalizations of Linear layers in the network.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – Alpha term used in the calculation of running mean and variance in batch normalization.</p></li>
<li><p><strong>L2_reg_term</strong> (<em>str</em>) – L2 regularization term used for penalizing larger weights in the network</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.add">
<span class="sig-name descname"><span class="pre">add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.add" title="Permalink to this definition"></a></dt>
<dd><p>Add layer: e.g. Linear or Activation</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dy</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.backward" title="Permalink to this definition"></a></dt>
<dd><p>Main method for Net in making a backward pass in backpropagation. Passes the gradient of the loss
function from the output layer to the input layer, which updates the weights and biases within the
Linear layers with the objective of minimizing the calculation of the Loss function in subsequent
iterations (i.e. improving model performance).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dy</strong> (<em>np.ndarray</em>) – passes the gradient of the loss function generated by the Loss criterion.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.forward" title="Permalink to this definition"></a></dt>
<dd><p>Main method for Net in making a forward pass in training / prediction. Iterates from input layer to
output layer each class’ forward function and passes forward predictions to use in calculating loss.
Predict is set to false by default to indicate training is occuring - this modifies the control-flow
within the Linear class, which specifically modifies the behaviour of batch normalization and dropout.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>np.ndarray</em>) – mini-batch x during forward pass of training or prediction.</p></li>
<li><p><strong>predict</strong> (<em>bool</em>) – True if performing a forward pass in prediction, or False if model is in training.
By default, set to false.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns output values of Net used for calculating Loss using the selected Criterion function</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.load_model">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.load_model" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_classes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.predict" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.reset_gradients">
<span class="sig-name descname"><span class="pre">reset_gradients</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.reset_gradients" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to zero out gradients of weights and biases in each layer, called before each iteration of training.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.save_model">
<span class="sig-name descname"><span class="pre">save_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.save_model" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.set_name">
<span class="sig-name descname"><span class="pre">set_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.set_name" title="Permalink to this definition"></a></dt>
<dd><p>Setter method for model name, useful for saving / loading models for re-use in making prediction or for continuing training</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.test_network">
<span class="sig-name descname"><span class="pre">test_network</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">test_set</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'test</span> <span class="pre">data'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.test_network" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.train_batch">
<span class="sig-name descname"><span class="pre">train_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.train_batch" title="Permalink to this definition"></a></dt>
<dd><p>Method that performs a training step on mini-batch x. Makes a forward pass, calculates loss and probabilities with
chosen loss criterion, then makes a backward pass to update the weights and biases of each Linear layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>np.ndarray</em>) – mini-batch x of training set during forward pass of the training phase</p></li>
<li><p><strong>label</strong> (<em>np.ndarray</em>) – ground-truth labels of mini-batch x, used for evaluating loss and accuracy</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns the calculated loss for the training iteration
np.ndarray: also returns the proportion of correctly predicted labels</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.train_convergence">
<span class="sig-name descname"><span class="pre">train_convergence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_set</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_set</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">report_interval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planned_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_check</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.train_convergence" title="Permalink to this definition"></a></dt>
<dd><p>Method for training model using an objective criteria that measures “convergence.” Two conditions are used to measure convergence:
(1) If training loss in a subsequent model is not below a selected N percentage threshold, then convergence is achieved; or
(2) If validation loss in chosen M subsequent models is not less than the last best model with minimal validation loss, then convergence is achieved.
Using these two disjunction of these two objectives, provides an approximate estimate for a good model using both the training loss
to check if loss is not decreasing enough, and validation loss to suggest that no better model can be found within reasonable time.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_set</strong> (<em>np.ndarray</em><em>, </em><em>np.ndarray</em>) – tuple whereby first index refers to training data and second index refers to training labels</p></li>
<li><p><strong>valid_set</strong> (<em>np.ndarray</em><em>, </em><em>np.ndarray</em>) – tuple whereby first index refers to validation data and second index refers to validation labels</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – size of each mini-batch used within each training iteration (each epoch)</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – a chosen percentage for the difference in training loss calculated each epoch to decide when to end training</p></li>
<li><p><strong>report_interval</strong> (<em>int</em>) – represents the number of epochs before each reporting interval. If 10 is set, a report is produced each 10 epochs in training.</p></li>
<li><p><strong>planned_epochs</strong> (<em>int</em>) – the maximum number of epoch to train model in the case that convergence criteria is not achieved within this time.</p></li>
<li><p><strong>last_check</strong> (<em>int</em>) – number of models to check after best model with minimum validation loss, if none have lower validation loss then convergence is achieved.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns the calculated loss for the training iteration
np.ndarray: also returns the proportion of correctly predicted labels</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.train_network">
<span class="sig-name descname"><span class="pre">train_network</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_set</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_set</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">report_interval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.train_network" title="Permalink to this definition"></a></dt>
<dd><p>Mini batch training.. separated from train that uses a dataloader which can also load batches, but
I think that it could be overkill and also doesn’t shuffle / take random samples like it should</p>
<p>This is not main train yet because not working well for low sizes, e.g. batch_size=1 or 2
however, just noticed it might be related to the log function in ce loss</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.update" title="Permalink to this definition"></a></dt>
<dd><p>Update step that uses the selected optimizer to update the weights and biases of each Linear layer, using
the differentials of the weights and biases within each layer (and parameters unique to the Optimizer algorithm)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.net.Net.validate_batch">
<span class="sig-name descname"><span class="pre">validate_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">valid_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.Net.validate_batch" title="Permalink to this definition"></a></dt>
<dd><p>Method that performs a validation step on mini-batch x. Makes a forward pass, calculates loss and probabilities with
chosen loss criterion, then makes a backward pass to update the weights and biases of each Linear layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>np.ndarray</em>) – mini-batch x of the validation set during forward pass of training</p></li>
<li><p><strong>label</strong> (<em>np.ndarray</em>) – ground-truth of labels of mini-batch x, used for evaluating loss and accuracy</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>returns the calculated loss for the training iteration
np.ndarray: also returns the proportion of correctly predicted labels</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="network.net.show_time">
<span class="sig-prename descclassname"><span class="pre">network.net.</span></span><span class="sig-name descname"><span class="pre">show_time</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.net.show_time" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="module-network.optim">
<span id="network-optim-module"></span><h2>network.optim module<a class="headerlink" href="#module-network.optim" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="network.optim.Adam">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">network.optim.</span></span><span class="sig-name descname"><span class="pre">Adam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-09</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.optim.Adam" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#network.optim.Optimizer" title="network.optim.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">network.optim.Optimizer</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="network.optim.Adam.add_to_dict">
<span class="sig-name descname"><span class="pre">add_to_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outdim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">i</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.optim.Adam.add_to_dict" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.optim.Adam.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.optim.Adam.step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="network.optim.Optimizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">network.optim.</span></span><span class="sig-name descname"><span class="pre">Optimizer</span></span><a class="headerlink" href="#network.optim.Optimizer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="network.optim.SGD">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">network.optim.</span></span><span class="sig-name descname"><span class="pre">SGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.04</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step_terms</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(10,</span> <span class="pre">0.5)</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.optim.SGD" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#network.optim.Optimizer" title="network.optim.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">network.optim.Optimizer</span></code></a></p>
<p>Stochastic Gradient Descent: Also implements weight decay (can be removed by setting to zero)</p>
<p>To add momentum etc. just adds another terms</p>
<dl class="py method">
<dt class="sig sig-object py" id="network.optim.SGD.exp_decay">
<span class="sig-name descname"><span class="pre">exp_decay</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.optim.SGD.exp_decay" title="Permalink to this definition"></a></dt>
<dd><p>Return decayed lr by an exponential factor of time and chosen constant k (exp term)
I have chosen k to factor in total training iters st lr magnitude pans over all planned epochs</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.optim.SGD.get_lr">
<span class="sig-name descname"><span class="pre">get_lr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.optim.SGD.get_lr" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.optim.SGD.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#network.optim.SGD.step" title="Permalink to this definition"></a></dt>
<dd><p>Update weights and biases in each layer wrt learned dW and db AND learning rate (+ weight decay) term</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.optim.SGD.step_decay">
<span class="sig-name descname"><span class="pre">step_decay</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.optim.SGD.step_decay" title="Permalink to this definition"></a></dt>
<dd><p>Return decreased lr by factor of “drop” (default: half) every “step” (default 10) epochs</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="network.optim.SGD.time_decay">
<span class="sig-name descname"><span class="pre">time_decay</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#network.optim.SGD.time_decay" title="Permalink to this definition"></a></dt>
<dd><p>Return decreased lr by time_step factor (epoch * iter)</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-network">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-network" title="Permalink to this headline"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="modules.html" class="btn btn-neutral float-left" title="network" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="network.dataset.html" class="btn btn-neutral float-right" title="network.dataset package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, COMP5329_Assignment_1.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>